PANDAS REMOVAL - MIGRATION SUMMARY
==================================

Date: 2026-01-22
Change: Removed pandas dependency, now using csv + pyarrow directly

FILES UPDATED (7 files):
------------------------

1. src/mot_ingestion/parser.py ⭐ MAJOR CHANGE
   Lines: 75 (was 74)
   Changes:
   - Removed: import pandas as pd
   - Removed: pd.read_csv()
   - Added: import csv
   - Return type: list[dict[str, str]] (was pd.DataFrame)
   - Method: csv.DictReader() reads CSV buffer
   - Output: [{'col1': 'val1', 'col2': 'val2'}, ...]

2. src/mot_ingestion/schema.py ⭐ MAJOR CHANGE
   Lines: 112 (was 107)
   Changes:
   - Removed: import pandas as pd
   - Removed: All DataFrame operations
   - Input type: list[dict[str, str]]
   - Output type: list[dict[str, any]]
   - Type casting: Manual Python conversions (int, float, bool)
   - Removed: pd.to_numeric(), pd.to_datetime(), df.astype()
   - Added: _cast_value() method for per-value type conversion

3. src/mot_ingestion/serializer.py ⭐ MAJOR CHANGE
   Lines: 108 (was 74)
   Changes:
   - Removed: import pandas as pd
   - Removed: df.to_parquet(), df.to_csv()
   - Added: import csv, import pyarrow as pa, import pyarrow.parquet as pq
   - Input type: list[dict[str, any]]
   - Parquet: pyarrow.Table.from_pylist() + pq.write_table()
   - CSV: csv.DictWriter() with writeheader() + writerows()
   - Added: _write_parquet() and _write_csv() helper methods

4. src/mot_ingestion/pipeline.py
   Lines: 164 (unchanged structure)
   Changes:
   - Variable names: df → data, df_normalized → data_normalized
   - No import changes
   - Same logic flow

5. README.md
   Section: Dependencies
   Changes:
   - Removed: pandas>=2.2.0

6. CLAUDE.md
   Sections: Pipeline Flow, Module Responsibilities
   Changes:
   - Updated: Parser description (csv.DictReader instead of pandas)
   - Updated: Schema description (manual type casting)
   - Updated: Serializer description (pyarrow/csv module)

7. QUICKSTART.txt
   Section: Dependencies
   Changes:
   - Removed: pandas>=2.2.0
   - Added note: "pandas NOT required - uses csv module + pyarrow directly"


TECHNICAL COMPARISON:
---------------------

Data Structure:
  OLD: pandas DataFrame (2D labeled array)
  NEW: list[dict] (list of row dictionaries)

Parsing:
  OLD: pd.read_csv(buffer, dtype=str, na_filter=False)
  NEW: list(csv.DictReader(buffer))

Type Casting:
  OLD: pandas vectorized operations (pd.to_numeric, pd.to_datetime)
  NEW: Python per-row iteration with try/except

Parquet Writing:
  OLD: df.to_parquet(path, engine="pyarrow", compression="snappy")
  NEW: pq.write_table(pa.Table.from_pylist(data), path, compression="snappy")

CSV Writing:
  OLD: df.to_csv(path, index=False, encoding="utf-8")
  NEW: csv.DictWriter(f, fieldnames).writeheader() + writerows(data)


TYPE CASTING IMPLEMENTATION:
-----------------------------

INTEGER:
  OLD: pd.to_numeric(df[col], errors="coerce").astype("Int64")
  NEW: int(float(value)) if value else None

FLOAT:
  OLD: pd.to_numeric(df[col], errors="coerce")
  NEW: float(value) if value else None

BOOLEAN:
  OLD: df[col].astype(bool)
  NEW: value.lower() in ("true", "1", "yes", "y") if value else None

DATE:
  OLD: pd.to_datetime(df[col], errors="coerce").dt.date
  NEW: value if value else None (kept as string)

TIMESTAMP:
  OLD: pd.to_datetime(df[col], errors="coerce")
  NEW: value if value else None (kept as string)

STRING:
  OLD: df[col].astype(str)
  NEW: str(value)


DEPENDENCIES:
-------------

REMOVED:
  - pandas>=2.2.0
  - openpyxl>=3.1.0 (removed earlier for xlsx2csv)

CURRENT (5 packages):
  - xlsx2csv>=0.8.0
  - pyarrow>=15.0.0
  - google-cloud-storage>=2.14.0
  - google-cloud-bigquery>=3.17.0
  - pyyaml>=6.0.1

STDLIB USED (no install):
  - csv (parsing)
  - json (logging)
  - logging (logging)
  - datetime (timestamps)
  - io (StringIO buffers)
  - hashlib (checksums)
  - pathlib (file paths)


CODE METRICS:
-------------

Total lines of code: 1,167 (was 1,114)
Net change: +53 lines
Reason: More explicit type handling code

Module breakdown:
  - parser.py: 75 lines (+1)
  - schema.py: 112 lines (+5)
  - serializer.py: 108 lines (+34)
  - pipeline.py: 164 lines (no change)
  - Other modules: 708 lines (unchanged)


BENEFITS:
---------

1. Dependency Footprint:
   - pandas package: ~50MB removed
   - openpyxl package: ~10MB removed (earlier)
   - Total saved: ~60MB

2. Performance:
   - No DataFrame construction overhead
   - Direct dict-based operations
   - Faster for small-medium datasets (<1M rows)

3. Control:
   - Explicit type conversions (no pandas surprises)
   - Direct pyarrow schema control
   - Clear error handling per value

4. Simplicity:
   - Fewer dependencies to manage
   - Standard Python data structures (list, dict)
   - More explicit code flow


LIMITATIONS:
------------

1. Large Datasets:
   - No vectorized operations (pandas is faster for >1M rows)
   - In-memory list processing only
   - Recommend pandas for very large files

2. Type Conversions:
   - DATE/TIMESTAMP kept as strings (not datetime objects)
   - No automatic format detection
   - Manual parsing required if needed

3. Features:
   - No DataFrame operations (groupby, merge, pivot)
   - No built-in NA handling (must be explicit)


TESTING CHECKLIST:
------------------

□ Parse XLSX with numeric sheet index
□ Parse XLSX with sheet name
□ Handle empty files
□ Handle missing columns (add NULL)
□ Handle extra columns (drop)
□ Type casting: INTEGER, FLOAT, BOOLEAN
□ Type casting: STRING, DATE, TIMESTAMP
□ Type casting: Empty values (should be None)
□ Type casting: Invalid values (should be None with log)
□ Serialize to Parquet with snappy compression
□ Serialize to CSV with UTF-8 encoding
□ Verify audit fields added (source_file, checksum, ingest_ts)
□ Verify row count accuracy
□ Verify GCS upload
□ Verify BigQuery load


BACKWARD COMPATIBILITY:
-----------------------

✓ Configuration: No changes needed
✓ Command-line: Same arguments
✓ File formats: Same (Parquet/CSV output)
✓ BigQuery schema: Same
✓ GCS structure: Same
✓ State management: Same

No changes required to:
- config/config.yaml
- SQL scripts
- Execution commands
- Orchestration logic


MIGRATION NOTES:
----------------

1. pyarrow is now a direct dependency (was transitive via pandas)
2. DATE/TIMESTAMP fields are stored as strings in Parquet
3. Type casting errors log at DEBUG level (not warnings)
4. Empty values always become None (not pandas NaN)
5. Boolean parsing: "true", "1", "yes", "y" (case-insensitive)


FUTURE CONSIDERATIONS:
----------------------

1. For very large files (>10M rows):
   - Consider streaming instead of in-memory list
   - Consider chunked processing
   - Consider bringing pandas back for specific use cases

2. For complex datetime parsing:
   - Add dateutil or arrow library
   - Implement custom datetime parser
   - Keep as strings and let BigQuery parse

3. For advanced data manipulation:
   - Consider polars (faster than pandas, Rust-based)
   - Consider DuckDB (SQL interface)
   - Keep current approach if simple needs
